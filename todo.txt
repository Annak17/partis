  - get germline vdj versions dynamically from naive data
  - will need to deal with productive and out-of-frame seqs separately, i.e. different parameters
  - move work/ out of Dropbox/
  - plots for gene choice probs as well
  - plot *all* the parameters
  - go back through and make it *super* clear where *all* the input parameters come from
  - account for different mutation frequencies to different bases
  - issue from erick: https://github.com/blab/immuno/issues/2
  - play around with naive data
  - get parameters from data more seamlessly, maybe with connor\'s stuff
      - get gene choice probs from *naive* and apply them to mature?
  - you\'re still writing some files for every query sequence in Stochhmm.cpp, I think. Don\'t!
  - if hamming distance is *very* small could precluster the *other* way, i.e. assume pairs are in the same event
  - would it be faster with STATE_MAX less than 1024?
  - why the hell is recombinator only spitting out clones with ten copies?
  - stop using imgt (get the versions dynamically, i.e. add an initial step to determine which alleles and versions this individual actually has)
  - I should really be able to construct the denominator in P(A,B) / (P(A) P(B)) from the numerator without recalculating much, right?
  - add assertion in text parser that emission and transition probs add to 1.0
  - rerun the version counter down to a much lower mean count
  - add in cdr3 length partitioning
  - for k_v --> k_v + 1, don\'t recalculate the whole table
  - TODOs in code
  - transitivity:
    - rather than assuming transitivity in clustering, couldn\'t I use the extra information to improve clustering?
        (NOTE this is roughly equivalent to representing an existing cluster by its viterbi path. hm, with mutations or not?)

  - low priority
    - rewrite parsing code/text files with http://www.yaml.org/
    - optimization
      - hmm structure optimization
        - it seems like I should be able to take advantage of the fact that all my hmms are super linear, i.e. the matrix of possible transitions is very sparse
        - implement banding? I think I have this listed below but without using the word 'banding'
      - try pairwise s-w for preclustering (all against all -- align against each *other* using sw)
      - blast instead of s-w?
      - when we\'re doing pairs, it seems there should be some optimizations due to the fact we don\'t need to actually identify the gene versions, but only partition the sequences (EDIT wait, what?)
      - alleles: save chunks of dp tables and reuse \'em
    - accuracy improvement
      - play around with s-w match/mismatch scores (match was 3[:1], just changed to 2[:1]... not sure what\'s best. It\'ll depend on the expected level of mutation. *sigh*)
      - the choices we make about which is the best reconstruction near the d/insertions is very dependent on how much mutation we think occurred
        - so try to use the amount of mutation in v to inform this decision (?)
        - i.e., calculate the within-sequence correlation between V mutation and D/insertion mutation
    - wait, what is stochhmm doing with the 'N's?
    - add bfloats to stochhmm?


# ----------------------------------------------------------------------------------------
misc
  - connor\'s spectral decomposition sorting
  
simulator
  - make *sure* that the mute freqs are being properly used by the tree simulator
  - use amalgamation of patients (and each individually, with mature or naive)
  - better insertion freqs (nucleotide choices, at least)
  - better SHM freqs distribution
  - do something with tree inference closure tests for recombinator (?)
  - TODOs in recombinator
  - tree branch lengths per-person?
  data/simulation differences
    we ignore infrequent vdj choices, i.e. data has a *huge* tail of vdj choices with only a few instances which is totally absent in simulation
    mutation frequencies aren\'t site-dependent in simulation (yet!)
    not really sure that I\'m treating the primers correctly a.t.m.
    the plots I made so far are only for the first thousand lines in the data files
hmmer
  - only use half of data for training
