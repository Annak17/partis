# ----------------------------------------------------------------------------------------
# abstract for bamfffff
#   hmm compiler
#   vdj inference
#   mention pair hmm
#   accuracy
#   speed  
# ----------------------------------------------------------------------------------------
  - compare other people\'s 'viterbi' codes
    - convert recombinator csv to fasta: /home/dralph/Dropbox/bin/csv2fasta
    - compare also with simulation restricted to only a few v genes, i.e. to get the average correct fraction higher
    - programs
      - imgt/vquest: http://www.imgt.org/IMGT_vquest/vquest
      - ihmmunealign  2.6 sec/seq
      - soda[12]
      - joinsolver
  - write shit up!
    - instructions: http://royalsocietypublishing.org/instructions-authors#question6

  - add datadir option to vdjalign
  - check shift_overflows in plotting.py
  - NOTE forbidding unphysical deletions seems to have roughly the same performance, while improving run time by a factor of 6
  - separate training and testing samples for performance comparison
  - clustering: do some heuristic preclustering, then for each of these clusters, use the n-hmm viterbi path, and cluster on these
  - stop pulling insert_mute_prob (in hmmwriter) ooya
  - rerun perf comparisons for partis/sw with new hamming_to_true_naive changes
  - need to validate two different things (i.e., I need to clean up and explicify my treatment of these things)
    - does the simulation accurately reflect the data?
    - does the hmm correctly infer parameters in simulation? (well, data too, but I dunno if there\'s really a way to test that)
  - think *long* and *hard* about get get_emission_prob() in hmmwriter. especially the pair part, since you didn\'t performance test that
    - UPDATE hm, on second thought I\'m just going to punt on the pair hmm stuff
    - TODO fix that!
  - account for different mutation frequencies to different bases
  - print a warning when the kbounds we pass to the hmm don\'t include the true one (and same thing for the gene versions)
  - synchronize replacement_gene treatment for *all* parameters between hmmwriter and recombinator
  - check how close parameters are between data and simulation
  - account for non-uniform base content in insertions
    - don\'t forget to put this into recombinator  
  - optimize on fuzzes and n_max_per_region
  - looking at the damn plots, my insertions and deletions are on average too small. Why is this?
  - go through all of jobholder making sure I\'m not using query_strs.first instead of both, or instead of choosing the proper one
  - add testing framework to partis (include timing info)
    - short reads, long reads, reads with v left and j right erosions
    - single viterbi, viterbi pair, clustering
    - parameter estimation from sw and hmm
  - stop using hackey versions of gtr.txt and trees.tre in recombinator. I.e. start inferring phylogenies
  - if hamming distance is *very* small could precluster the *other* way, i.e. assume pairs are in the same event
  - TODOs in code

  - low priority
    - will need to deal with productive and out-of-frame seqs separately, i.e. different parameters
    - disallow stop codons, out of frame and frame-shifted rearrangements in recombinator
    - do I really want to apply the gene choice prob in waterer?
    - optimization
      - would it be faster with STATE_MAX less than 1024?
        - for k_v --> k_v + 1, don\'t recalculate the whole table
      - run performance profiling
      - adding the gobbledygook states seems to have *really* slowed it down (EDIT oh, wait, that was just removing the gcc optimization options)
      - hmm structure optimization
        - it seems like I should be able to take advantage of the fact that all my hmms are super linear, i.e. the matrix of possible transitions is very sparse
        - implement banding? I think I have this listed below but without using the word 'banding'
      - try pairwise s-w for preclustering (all against all -- align against each *other* using sw)
    - I should really be able to construct the denominator in P(A,B) / (P(A) P(B)) from the numerator without recalculating much, right?
      - blast instead of s-w?
      - when we\'re doing pairs, it seems there should be some optimizations due to the fact we don\'t need to actually identify the gene versions, but only partition the sequences (EDIT wait, what?)
      - alleles: save chunks of dp tables and reuse \'em
    - accuracy improvement
      - play around with s-w match/mismatch scores (match was 3[:1], just changed to 2[:1]... not sure what\'s best. It\'ll depend on the expected level of mutation. *sigh*)
      - the choices we make about which is the best reconstruction near the d/insertions is very dependent on how much mutation we think occurred
        - so try to use the amount of mutation in v to inform this decision (?)
        - i.e., calculate the within-sequence correlation between V mutation and D/insertion mutation
    - clustering
      - use an N-HMM instead of a pair-HMM plus pairscore clustering
      - neighbor-joining
      - transitivity:
        - rather than assuming transitivity in clustering, couldn\'t I use the extra information to improve clustering?
            (NOTE this is roughly equivalent to representing an existing cluster by its viterbi path. hm, with mutations or not?)

# ----------------------------------------------------------------------------------------
misc
  - connor\'s spectral decomposition sorting
  
simulator
  - make *sure* that the mute freqs are being properly used by the tree simulator
  - use amalgamation of patients (and each individually, with mature or naive)
  - better insertion freqs (nucleotide choices, at least)
  - better SHM freqs distribution
  - do something with tree inference closure tests for recombinator (?)
  - TODOs in recombinator
  - tree branch lengths per-person?
  data/simulation differences
    we ignore infrequent vdj choices, i.e. data has a *huge* tail of vdj choices with only a few instances which is totally absent in simulation
    mutation frequencies aren\'t site-dependent in simulation (yet!)
    not really sure that I\'m treating the primers correctly a.t.m.
    the plots I made so far are only for the first thousand lines in the data files
hmmer
  - only use half of data for training
